Tree-structured Parzen Estimators (TPE)
개요
Tree-structured Parzen Estimators (TPE)는 Gaussian Process의 단점을 수정합니다. 각 반복 TPE는 새로운 관찰을 수집하고 반복의 끝에서 알고리즘은 다음에 시도해야하는 매개 변수 집합을 결정합니다. 주요 아이디어는 비슷하지만 알고리즘은 완전히 다릅니다.

맨 처음에는 하이퍼파라미터에 대한 사전 분포를 정의해야합니다. 기본적으로 모두 균등하게 분포 될 수 있지만 임의의 하이퍼파라미터를 임의의 단일 모달 분포와 연관시킬 수 있습니다.

처음 몇 번의 반복에서는 TPE 알고리즘을 warn up(워밍업)해야합니다. TPE를 적용하려면 먼저 데이터를 수집해야합니다. 가장 쉽고 간단한 방법은 Random Search을 몇 번 반복하는 것입니다. Random Search에 대한 반복 횟수는 TPE 알고리즘에 대해 사용자가 정의한 매개 변수입니다.

우리가 데이터를 수집 할 때 마침내 TPE를 적용 할 수 있습니다. 다음 단계는 수집 된 관측치를 두 그룹으로 나누는 것입니다. 첫 번째 그룹은 평가 후 최고 점수를 준 관측치를 포함하며, 두 번째 그룹은 다른 모든 관측치를 포함합니다. 그리고 목표는 첫 번째 그룹에 있을 가능성이 높고 두 번째 그룹에 있을 가능성이 적은 일련의 매개 변수를 찾는 것입니다. 최상의 관측치의 일부는 사용자가 TPE 알고리즘의 매개 변수로 정의합니다. 일반적으로 관측치의 10-25 %입니다.

 

보시다시피 더 이상 최선의 관찰에 의존하지 않습니다. 대신, 우리는 최상의 관측치 분포를 사용합니다. Random Search에서 우리가 사용하는 반복이 많을수록 처음에는 더 나은 분포를 보입니다.

TPE의 다음 부분은 두 그룹 각각에 대한 우도 확률()을 모델링하는 것입니다. 이것은 Gaussian Process와 TPE 간의 다음 큰 차이입니다. Gaussian Process의 경우 우도 확률 대신 사후 확률을 모델링했습니다. 첫 번째 그룹 (최선의 관측치를 포함하는 그룹)의 likelihood probability을 사용하여 후보자를 샘플링합니다. 샘플링 된 후보들로부터 우리는 첫 번째 그룹에있을 확률이 높고 두 번째 그룹에있을 가능성이 적은 후보자를 찾습니다. 다음 수식은 각 후보자마다 예상되는 개선을 정의합니다l
EI(x)=  (l(x))/(g(x))
l(x): 첫번째 그룹에 있을 확률
g(x): 두번째 그룹에 있을 확률
다음은 그 예입니다. 두 그룹에 대해 미리 정의 된 분포가 있다고 가정 해 봅시다. 그룹 # 1에서 6 명의 후보자를 추출합니다. 그리고 각각에 대해 Expected Improvement를 계산합니다. 가장 큰 개선이 있는 매개 변수는 다음 반복에 사용할 매개 변수입니다.

 

이 예제에서는 t- 분포를 사용했지만 parzen-window density estimators를 사용하는 TPE 분포 모델에서 사용했습니다. 주요 아이디어는 각 샘플이 지정된 평균 (하이퍼파라미터의 값) 및 표준 편차로 가우스 분포를 정의한다는 것입니다. 그런 다음이 모든 점들이 서로 겹치고 표준화되어 출력이 확률 밀도 함수 (PDF)가 되도록합니다. Parzen estimators가 알고리즘의 이름으로 나타나는 이유입니다
참고) parzen-window density estimators

밀도추정(density estimation) 방법은 크게 parametric 방법과 non-parametric 방법으로 구분할 수 있다.

Parametric 밀도추정은 미리 pdf(probability density function)에 대한 모델을 정해놓고 데이터들로부터 모델의 파라미터만 추정하는 방식이다. 예를 들어, '일일 교통량'이 정규분포를 따른다고 가정해 버리면 관측된 데이터들로부터 평균과 분산만 구하면 되기 때문에 밀도추정 문제가 비교적 간단한 문제가 되어 버린다.

그런데 현실 문제에서 이렇게 모델이 미리 주어지는 경우는 많지 않으며 분포의 모델을 미리 안다는 것은 너무나 강한 혹은 사치스러운 가정일 수 있다. 이 경우 어떠한 사전 정보나 지식 없이 순수하게 관측된 데이터만으로 확률밀도함수를 추정해야 하는데 이를 non-parametric density estimation라 부른다.

Non-parametric 밀도추정의 가장 간단한 형태가 바로 히스토그램(histogram)이다. 즉, 관측된 데이터들로부터 히스토그램을 구한 후 구해진 히스토그램을 정규화하여 확률밀도함수로 사용하는 것이다.


Non-parametric 밀도추정은 다음과 같이 공식화 할 수 있다.
 
여기에는 2가지 접근 방법이 있는데, 
1. kernel density estimation (KDE): fix 𝑉 and determine 𝑘 from the data
2. k-nearest-neighbor (kNN): fix 𝑘 and determine 𝑉 from the data
2가지 접근법 모두 N이 무한대로 수렵할수록, 실제확률 밀도 함수에 수렴한다.(V가 적절히 축소되고, k가 적절이 커진다면)

KDE중 하나가 Parzen windows 이다.
 
예를 둘러싸는 영역 𝑘은  x을 중심으로한  길이 ℎ 변을 가지는 하이퍼 큐브라고 가정해보자.
따라서 V = h^D 이다 (D는 차원의 수, V는 부피).
이 영역에 속하는 예제들의 수(k)를 찾기 위해 우리는 커널 함수 𝐾(𝑢)를 정의한다.
 
• 원점에 중심을 둔 unit 하이퍼 큐브에 해당하는이 커널은 Parzen-window 또는 naïve estimator로 알려져 있다
• 𝑥 (𝑛가 ℎ를 중심으로하는 측면 ℎ의 하이퍼 큐브 내부에 있으면 수량 𝐾( (𝑥 – 𝑥(𝑛 ) / ℎ)은 1 이고 그렇지 않으면 0이다.

하아퍼큐브 안에 있는 포인트의 수는 다음과 같다.
 

이러한 표현을 p(x) ~= k/(NV) 식에 대입하면 다음과 같다.
 

http://research.cs.tamu.edu/prism/lectures/pr/pr_l7.pdf?bcsi_scan_f48ea128b316325d=0&bcsi_scan_filename=pr_l7.pdf
http://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture6.pdf?bcsi_scan_f48ea128b316325d=0&bcsi_scan_filename=Lecture6.pdf
http://carstart.tistory.com/

.



그리고 트리 구조(tree-structured)는 매개 변수 공간이 트리의 형태로 정의된다는 것을 의미합니다. 나중에 우리는 네트워크에 가장 적합한 수의 레이어를 찾으려고 노력할 것입니다. 여기서는 하나 또는 두 개의 hidden 레이어를 사용하는 것이 더 나은지 여부를 결정하려고 합니다. 두 개의 hidden 레이어를 사용하는 경우 첫 번째 및 두 번째 레이어의 hddien units 수를 독립적으로 정의해야합니다. 하나의 hidden 레이어를 사용하는 경우 두 번째 hidden 레이어의 units 수를 정의 할 필요가 없습니다. 지정된 매개 변수 집합에 존재하지 않기 때문입니다. 기본적으로 두 번째 숨겨진 레이어의 숨겨진 유닛 수는 숨겨진 레이어의 수에 따라 다릅니다. 즉, 매개 변수는 트리 구조 종속성을 갖습니다.

단점
이 알고리즘의 가장 큰 단점은 매개 변수를 서로 독립적으로 선택한다는 것입니다. 예를 들어, 정규화와 훈련 epoch 매개 변수의 수 사이에는 명확한 관계가 있습니다. 정규화를 통해 우리는 일반적으로 더 많은 epochs을 위해 네트워크를 훈련 할 수 있으며 더 많은 epochs으로 더 좋은 결과를 얻을 수 있습니다. 반면에 정규화가없는 경우 네트워크가 과도하게 시작되고 유효성 검사 오류가 증가하기 때문에 많은 epochs이 나쁜 선택이 될 수 있습니다. 정규화 변수의 상태를 고려하지 않고서는, 에포크의 수에 대한 각각의 다음 선택이 임의로 보일 수 있습니다.

출처 및 참고
http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#bayesian-optimization
http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf

hyperopt document
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.704.3494&rep=rep1&type=pdf
